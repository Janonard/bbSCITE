\section{Move Proposals}
\label{sec:move_proposal}

Another area of concern in the design of \ac{ffSCITE} was the sampling of random experiments to propose state changes. On multiple occasions in the \textsc{ChainStep} algorithm (Algorithm \ref{alg:scite-step}), parameters of later actions are sampled uniformly from a set of options and a Bernoulli trial decides whether the newly proposed state is accepted as the new current state. These samples require a source of randomness, and Jahn et al. \cite{tree2016} used the \texttt{rand} function from the standard C library for their reference implementation. This function is however not available on \acp{FPGA}, so we had to use other methods. The C++ standard library provides a framework for handling random numbers distinguishes \acp{URNG}, which are pseudo-random algorithms that sample uniformly from a set of integers fixed by their design, and distributions, which are algorithms that take the output of any \ac{URNG} to provide a sample of the requested distribution. This may be a uniform distribution from a smaller or bigger set than the original output, but it may also be a Bernoulli trial or a normal distribution. The C++ standard library often provides only a single class for every distribution and since those that are relevant to us work just fine on \acp{FPGA}, we did not have to deal with this issue.

Deciding on a \ac{URNG} however was a big decision: There are different generators in the C++ standard library. These \acp{URNG} have different statistical properties, but some are more or less practical as a chip design, at least out of the box. As it is common practice, we decided to initially use a simple \ac{URNG} that may or may not be perfect for our use case and to exchange it once we encountered problems either with its output quality or performance. This \ac{URNG} is \texttt{oneapi::dpl::minstd\_rand}, an implementation of the Minstd \ac{URNG} \cite{park1988random} provided by Intel in the OneDPL library; A subset of the C++ standard library designed especially for and with OneAPI. Park and Miller, the authors of Minstd, advertised it with the words ``We believe that this is the generator that should always be used- unless one has access to a random number generator known to be better.'' \cite{park1988random} Since the implementation also does not have any internal loops which could cause structural problems, this \ac{URNG} was a good first choice for us. Other, more popular \acp{URNG} like the Mersenne Twister \cite{matsumoto1998mersenne} did not meet this criterion and where therefore discarded for our initial implementation. Finding a better \acp{URNG}, however, proved to be harder than expected. We found it hard for an outsider to identify the state of the art in the field and since \ac{ffSCITE} already passes our quality benchmarks with Minstd, we decided to prioritize other, more pressing issues discussed in the previous sections and keep working with Minstd.

\todo[inline]{The \ac{URNG} from the quoted paper is known as Minstd0. The \ac{URNG} I'm actually using is Minstd, which uses a different coefficient and was published by the same authors. However, it was not published as a peer-reviewed paper, but as a letter to the editor. How do I cite this? \url{http://www.firstpr.com.au/dsp/rand31/p105-crawford.pdf\#page=4}}

There are however problems with Minstd that we had to work around: Minstd is a Lehmer random number generator \cite{lehmer1951mathematical}. This means that given a previous output or seed $X_{k}$, its next output is defined as $X_{k+1} = a \cdot X_k \mod m$ for well-chosen parameters $a$ and $m$. In the case of Minstd, these are $a = 48271$ and $m = 2^{32} - 1 = 2147483647$. An implementation of Minstd, therefore, needs to work with integers wider than 32 bits, and since \texttt{oneapi::dpl::minstd\_rand} is designed to work on \acsp{CPU}, \acsp{GPU} and \acsp{FPGA}, it uses 64-bit integers. 64-bit modulo operations are however \emph{very} resource-intensive on \acp{FPGA} and require multiple cycles to complete. Lastly, the output of Minstd is supposed to appear random and is fed back as the new state of the generator. It is therefore impossible to predict the new state of the \ac{URNG} and any loop that draws from a \ac{URNG} has a data dependency that increases its \ac{II} significantly. We were not able to solve this problem as it would require researching or developing another \ac{URNG} and, therefore, needed work around the issue by isolating the move proposals to a custom kernel, reducing the total calls to the \ac{URNG}, and using multiple independent \ac{URNG} instances to reduce the critical path.