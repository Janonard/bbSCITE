\section{Implementation and Evaluation}
\label{sec:evaluation}

\ac{ffSCITE} is implemented as a \ac{HLS} design written in C++20, using SYCL and Intel OneAPI version 22.3.0. %, which internally uses Intel Quartus version 20.4.0 Build 72 Pro. 
The target \ac{FPGA} is a Intel Stratix 10 GX 2800 \ac{FPGA} on a Nallatech 520N PCIe card. The utilized backend tool chain is Intel Quartus Pro version 20.4.0 Build 72, matching the version of the board support package (\emph{20.4.0\_hpc}) that provides PCIe and DDR memory interfaces. 
%The target \ac{FPGA} is a Intel Stratix 10 GX 2800 \ac{FPGA} on a Nallatech 520N, with the 20.4.0 HPC board support package.

\subsection{Hardware synthesis}
\label{subsec:hardware}


\begin{table*}[tbh]
    \centering
    \begin{tabular}{rllllllllll}
        \toprule
        \textbf{Kernel/Component} & \multicolumn{2}{l}{\textbf{Look-up tables}} & \multicolumn{2}{l}{\textbf{Flip-Flops}} & \multicolumn{2}{l}{\textbf{RAM blocks}} & \multicolumn{2}{l}{\textbf{MLABs}} & \multicolumn{2}{l}{\textbf{DSPs}} \\
        \cmidrule(r){1-1} \cmidrule(l){2-3} \cmidrule(l){4-5} \cmidrule(l){6-7} \cmidrule(l){8-9} \cmidrule(l){10-11}
        & \textbf{64-bit} & \textbf{96-bit} & \textbf{64-bit} & \textbf{96-bit} & \textbf{64-bit} & \textbf{96-bit} & \textbf{64-bit} & \textbf{96-bit} & \textbf{64-bit} & \textbf{96-bit} \\
        Static Partition & 24.4\% & 24.4\% & 24.4\% & 24.4\% & 22.4\% & 22.4\% & 0.0\% & 0.0\% & 18.2\% & 18.2\% \\
        Interconnect & 1.2\% & 1.2\% & 0.7\% & 0.7\% & 1.0\% & 0.9\% & 0.0\% & 0.0\% & 0.0\% & 0.0\% \\
        IO Kernel & 0.7\% & 0.6\% & 0.6\% & 0.6\% & 0.8\% & 0.6\% & <0.1\% & <0.1\% & <0.1\% & <0.1\% \\
        Worker Kernel & 16.4\% & 27.7\% & 10.5\% & 13.6\% & 6.5\% & 10.8\% & 1.0\% & 0.6\% & 9.8\% & 12.6\% \\
        % Blocks B8, B9
        \textit{(thereof ``Execute Move'')} & \textit{(9.2\%)} & \textit{(13.6\%)} & \textit{(5.4\%)} & \textit{(6.3\%)} & \textit{(3.0\%)} & \textit{(2.9\%)} & \textit{(0.6\%)} & \textit{(0.3\%)} & \textit{(8.0\%)} & \textit{(10.8\%)} \\
        \textbf{Total} & \textbf{42.7\%} & \textbf{53.9\%} & \textbf{36.2\%} & \textbf{39.4\%} & \textbf{30.7\%} & \textbf{34.9\%} & \textbf{1.0\%} & \textbf{0.6\%} & \textbf{28.0\%} & \textbf{30.8\%} \\
        \bottomrule
    \end{tabular}
    \Description{A table with the resource usages of the different parts of the two designs. The listed resources are look-up tables, flip-flops, RAM blocks, MLABs, and DSPs, and the listed parts are the static partition, the interconnect, the IO kernel, the worker kernel, and thereof the ``Execute Move'' block.}
    \caption{Resource usage of the 64-bit and 96-bit design instances. The 64-bit instance is clocked at 307.50 MHz, and the 96-bit instance is clocked at 316.67 MHz. Percentages are relative to available resources on the Intel Stratix 10 GX 2800 \ac{FPGA} (1866240 look-up tables, 3732480 flip-flops, 11721 RAMs, $\sim$133900 MLABs, 5760 DSPs). The static partition provides the interface to the host and is fixed by the card vendor.}
    \label{tab:ressource_usage96}
\end{table*}

All components inside the worker kernel (Sec.~\ref{sec:kernel_structure}) are implemented as pipelined loops with an \ac{II} of 1 that iterate over the tree nodes. The maximal number of nodes in a tree is fixed as a design parameter during compilation. This means that the number of bits in a bit-vector, the number of vectors in an ancestor matrix, and therefore also the number of iterations and the width of the data path are fixed with this parameter. The biggest vector size that we have found to be reliably synthesizable for this target is 96 bits; The resource usage for such a design instance as well as for a smaller 64-bit variant is listed in Tab.~\ref{tab:ressource_usage96}. Both designs are relatively logic-intensive, on the one hand due to many bit-level operations, mostly in the ``Execute Move'' loop, and on the other hand to orchestrate movement of the bit-matrices. The floating-point \acp{DSP} of the Stratix 10 architecture see some usage for the arithmetic parts of the likelihood calculation, and not many \acl{BRAM} are used. Despite different resource usage, both designs achieve similar clock frequencies around 310 MHz.

The bottleneck for larger vector sizes is routing. When trying to compile the design with 128-bit vectors with an unlimited clock speed, the demand for interconnect wires peaks up to 125\%. This is only eliminated by lowering the clock speed to the regions around 150 MHz, which is less than a third the maximum clock speed of 480 MHz for the Intel Stratix 10 \ac{FPGA} and less than half the clock speeds of the smaller design instances. Even with a lowered clock speed, timing errors still occur and we were thus unable to reliably synthesize the design with 128 bits. There also seems to be an issue with the arbitrary precision integer type\footnote{\url{https://www.intel.com/content/www/us/en/docs/oneapi-fpga-add-on/optimization-guide/2023-0/var-prec-fp-sup.html}} that we use to implement bit vectors. The last bit of the 96-bit vector is always set to zero and thus limits the usable number of nodes in our experiments to 95. The problem does not show up with 32-bit, 64-bit, or spurious 128-bit design instances.


\subsection{Performance benchmark}
\label{subsec:performance}

In order to compare the performance of \ac{ffSCITE} to the reference \ac{SCITE} implementation by \citeauthor{tree2016} \cite{tree2016}, we measure the throughput in Markov chain steps per second.
%Our main goal is to provide an implementation of \ac{SCITE} that has a higher throughput in Markov chain steps per second than the reference implementation by \citeauthor{tree2016} \cite{tree2016}. We have set up the following benchmark to verify that we have met this goal: 
For this benchmark, we have generated random input data with different sizes, using the stochastic assumptions presented in Section \ref{sec:introduction} and arbitrary error probabilities. We wanted to test both \ac{ffSCITE} variants with fully and partially utilized bit vectors and therefore chose 32 bits, 64 bits, and 95 bits as input sizes. For 32 bits, the input covers 32 cells and 31 genes so that the mutation tree has 32 nodes. Other input sizes are analogous, and we had to use 95 bits as the biggest input size since the 96th bit of 96-bit \ac{ffSCITE} is faulty (see Subsection \ref{subsec:hardware}). Then, we simulated 48 Markov chains with 2,000,000 chain steps each for every input and measured the runtime, as well as the power draw of the \ac{ffSCITE} instances. We repeated this experiment four times and took the average of those repetitions as the final value.

\begin{table}[tbh]
    \begin{tabular}{crrrcc}
        \toprule
        \textbf{Input Size}
        & \multicolumn{3}{c}{\textbf{Throughput [ksteps/s]}} 
        & \multicolumn{2}{c}{\textbf{Speedup}} \\
        \cmidrule(r){1-1}
        \cmidrule(r){2-4}
        \cmidrule(r){5-6}
        \textbf{Cells $\times$ Genes}
        & \textbf{CPU} 
        & \multicolumn{2}{c}{\textbf{FPGA design}}
        & \multicolumn{2}{c}{\textbf{FPGA/CPU}} \\
        \cmidrule(r){3-4}
        \cmidrule(r){5-6}
        & & \textbf{96-bit} & \textbf{64-bit} & \textbf{96-bit} & \textbf{64-bit} \\
        32 $\times$ 31 & 229.87 & 4891.25 & 7486.57  & 21$\times$ & 33$\times$ \\
        64 $\times$ 63 &  70.65 & 4123.53 & 4349.70  & 58$\times$ & 62$\times$ \\
        95 $\times$ 94 &  36.55 & 2938.25 & n/a      & 80$\times$ & n/a \\
        \bottomrule
    \end{tabular}
    \Description{A table listing the throughput of the CPU reference as well as the 96-bit and 64-bit designs in ksteps/s for different input sizes. The input sizes are 32 cells and 31 genes, 64 cells and 63 genes, and 95 cells and 94 genes. Additionally, the speedup of the FPGA designs relative to the CPU reference is listed.}
    \caption{Mean throughput of SCITE (single thread on AMD EPYC Milan 7763 CPU) and two variants of \ac{ffSCITE} (Stratix 10 GX 2800 FPGA) for different input sizes. 64-bit \ac{ffSCITE} is not able to process mutation matrices with more than 63 genes since the bit vector width limits the number of genes it can process. 
    %96-bit \ac{ffSCITE} constantly draws 75.17 W and 64-bit \ac{ffSCITE} constantly draws 72.42 W.
    }
    \label{tab:throughput}
\end{table}

This performance data is shown in Tab.~\ref{tab:throughput}, where we have listed the throughput of the three executables in executed Markov chain steps per second. We see that both \ac{ffSCITE} variants are significantly faster than the CPU-based \ac{SCITE} implementation, with speedups of up to 80$\times$. We also see that the speedup grows %nearly linearly 
with the input size and that the smaller \ac{ffSCITE} variant achieves a higher throughput for the inputs it can process than the bigger variant, despite slightly lower clock frequency. This means that if a user knows that their input will not exceed a certain synthesizable size, it can be beneficial to only build \ac{ffSCITE} with this bit-vector length.

\subsection{Performance model and pipeline latency}

\begin{table}[tbh]
    \begin{tabular}{cccccc}
        \toprule
        \textbf{Instance} & \textbf{Input Size} & \multicolumn{2}{c}{\textbf{Throughput [ksteps/s]}} & \textbf{Acc.} \\
        \cmidrule(r){1-1} \cmidrule(r){2-2} \cmidrule(r){3-4} \cmidrule(r){5-5}
        & \textbf{Cells $\times$ Genes} & \textbf{Measured} & \textbf{Modelled} \\
        96-bit & 32 $\times$ 31 & 4891.25 & 9596.06 & 51.0\% \\
        & 64 $\times$ 63 & 4123.53 & 4871.85 & 84.6\% \\
        & 95 $\times$ 94  & 2938.25 & 3298.65 & 89.1\% \\
        64-bit & 32 $\times$ 31 & 7486.57 & 9318.18 & 80.3\% \\
        & 64 $\times$ 63 & 4349.70 & 4730.77 & 91.9\% \\
        \bottomrule
    \end{tabular}
    \Description{A table listing the measured throughput and the model prediction for the different design instances and input sizes.}
    \caption{Comparison of measured and modeled throughput for different design instances and input sizes, including the model accuracy.}
    \label{tab:model}
\end{table}

The throughput of \ac{ffSCITE} can be modeled by the number of iterations in its pipelined loops, ideally consuming one cycle per iteration. The slowest of these loops transfers the current state of a chain between IO and worker kernel.
%The throughput of \ac{ffSCITE} is bounded by its design with separate IO and worker kernels: 
%The current state of a chain is transferred between these kernels via pipes, but this state is too big to be transferred in one clock cycle. 
Each state is therefore transferred in packets, row by row. With $n$ denoting the number of nodes in the mutation tree (i.e. the number of utilized bit-vector bits and rows in an ancestor matrix), then the first $n$ packets contain a tuple with a row of the ancestor matrix and a row of the descendant matrix, followed by one packet with meta information. In total, there are $n+1$ packets per state, which takes $n+1$ cycles to be transferred. The other pipelined loops, i.e. the likelihood calculation and the mutation tree updates, should be slightly faster at $n$ cycles.
Thus, the throughput of the pipe in steps per second is upper bounded by $f/(n+1)$ in steps per second, where $f$ is the clock frequency of the final design.

We compare the measured throughput of \ac{ffSCITE} with this model in Tab.~\ref{tab:model}. The model matches fairly well for inputs that fully utilize the available bit-vectors, but the model accuracy drops off for smaller inputs. We suspect that this might be due to the overall latency of the worker kernel: The IO kernel currently assumes a fixed latency of the worker kernel, measured in chain states, which does not take the input size into account. However, the worker's latency in processed chain states might be bigger for smaller input sizes. This would mean that the IO kernel needs to stall until the results are sent back from the worker, which would explain the lowered throughput. This hypothesis could be investigated with further experiments. %Unfortunately, we have not properly tested this hypothesis yet. 
The presented designs are based on a coarse empirical design space exploration, which yielded 24 independent chain states to hide the worker latency.
%We have only made a coarse design space exploration to find the worker latency with the highest throughput, which yielded 24 chain states as the best approximation. 
The good correspondence of measurements and model for full inputs confirms that this choice is reasonable for this case, but may need refinement for other input sizes.
%However, the good accuracy of the model for full inputs is enough for us to use this model as an idealized upper bound of \ac{ffSCITE}'s performance.