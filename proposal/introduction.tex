\section{Motivation and Goal of the Thesis}

Cancer is a widespread and often lethal disease\cite{10.1001/jamaoncol.2021.6987} where body cells mutate in a way that increases their cell division speed as well as their lifetime while also evading immune responses. There are many possible treatment methods like surgery, chemotherapy, or radiation therapy, but their effectiveness often depends on the exact type of tumor. Tumors however are not homogenous: Individual tumor cells may mutate again and form new subclones that compete against others \cite{nik2012life}. Treating the dominant subclones however seems to provide an advantage for previously minor, resistant subclones \cite{gillies2012evolutionary}. Therefore, knowledge of existing subclones and their evolutionary history may help future treatments \cite{greaves2012clonal}\cite{stratton2009cancer}\cite{swanton2012intratumor}.

Bulk sequencing of tumor cells and analysing the found mutations appears to be already widely used, but often misses out on smaller, upcoming variants since they are averaged out in the mass of cells \cite{navin2014cancer}. Therefore, research has been done to utilise single cell sequencing. With this technique, the exact genome of individual cells can be identified and compared against other cells. However, it also comes with high error rates and parts of a cell's genome are often lost during the process \cite{tree2016}. It is therefore very hard to compute an exact phylogenetic tree with this data and one may to resort to find the most plausible one instead.

One algorithm to compute the most likely phylogeny from a set of erroneous cell genomes is \ac{SCITE}\cite{tree2016}. The original authors provided a functional, but rather unoptimized implementation of their algorithm. While we know of an optimized but unpublished implementation of \ac{SCITE} for Intel x86 CPUs, there has not been an efficient implementation for \acp{FPGA}, at least to our knowledge. \acp{FPGA} are computer chips that contain a lattice of logic, computation, and memory units that are connected via programmable connections. They basically emulate a complete computer chip and have already been used extensively for chip prototyping and verification \cite{rodriguez2007features}. In recent years they have also become interesting as computation accelerators for \ac{HPC} users due to their low power consumption compared to their performance, but developing efficient \ac{FPGA} designs is tedious since compiling them often takes multiple hours and naive designs are often multiple orders of magnitudes worse than optimized ones \cite{betkaoui2010comparing}. 

However, with the introduction of Intel oneAPI\footnote{\url{https://www.intel.com/content/www/us/en/developer/tools/oneapi/overview.html}}, it has become possible to develop designs for Altera/Intel \acp{FPGA} using C++ templates, a form of polymorphism that is very suitable for \ac{FPGA} designs: Datatypes and functions are implemented with placeholders that can be replaced with actual types and constants during compilation. When set up correctly, one can therefore implement different solutions to the same problem, integrate the different solutions in the build system, compile the different versions in parallel and evaluate the resulting designs side by side. We have already employed this method while developing StencilStream\footnote{\url{https://github.com/pc2/StencilStream}}, a Generic Stencil Simulation Library for \acp{FPGA}, and it significantly improved our productivity.

Our goal for this thesis is therefore to develop an efficient implementation of SCITE for \acp{FPGA} using oneAPI and the aforementioned development pattern. We expect this implementation to be faster than the original author's implementation, and possibly even faster as the optimized CPU version mentioned previously. With this, we also want to showcase the increased productivity of \ac{FPGA} development with oneAPI; That even an undergraduate student can develop efficient FPGA designs in the limited amount of time of a Bachelor's Thesis.

\section{Problem Formulation}

The input is a function $D: C \times G \rightarrow \{0, 1, ?\}$, where $C$ is the set of sampled cells and $G$ is a set of genome positions. If for a cell $c \in C$ and a genome position $g \in G$ we have $D(c, g) = 1$, the genome position $g$ has been observed as mutated in cell $c$, and if we have $D(c, g) = 0$, the genome position $g$ has been observed in it's common form. There is also a third option $D(c, g) = ?$, which means that the genome position has not been found at all and no statement could be made whether it is mutated or not. As the existance of the $?$ option may indicate, this function is noisy and may contain errors. It is assumed that errors occur independently from each other and if we let $E: C \times G \rightarrow \{0, 1\}$ be the true state of the cells, we have
\begin{align*}
    \mathbb{P}(D(c, g) = 0 \mid E(c, g) = 0) &= 1 - \alpha & \mathbb{P}(D(c, g) = 1 \mid E(c, g) = 0) &= \alpha \\
    \mathbb{P}(D(c, g) = 0 \mid E(c, g) = 1) &= \beta & \mathbb{P}(D(c, g) = 1 \mid E(c, g) = 1) &= 1 - \beta
\end{align*}
for some error rates $\alpha, \beta \in (0, 1)$. Error modeling for $D(c, g) = ?$ is not given by the authors; Missing data is always ignored in probability calculations.

The goal now is to find a mutation tree, a cell attachment and error rates with the maximum likelihood of being correct. A mutation tree is a rooted tree $T = (G \cup \{r\}, E, r)$ where $r$ is the root of the tree and the other nodes are genome positions. It expresses that the mutation of $g \in G$ has occured after all it's ancestors, prior to it's successors and independently from all unrelated genome positions. A cell attachment is a simple function $\sigma: C \rightarrow G$ and expresses that a cell $c \in C$ has the mutation status just after $\sigma(c)$ was mutated. Therefore, a cell $c$ has mutations at all genome positions that are ancestors of $\sigma(c)$, but at no other positions. This model is sensible since the authors of \ac{SCITE} make the ``infinite sites'' assumption, which states that a genome position mutates exactly once in the complete history of the tumor and does not mutate back.

With the mutation tree $T$ and a cell attachment $\sigma$, we can now build a function $e_{(T, \sigma)}: C \times G \rightarrow \{0, 1\}$ that expresses the made statements of the cell's true mutation status. Formally, we set
\begin{align*}
    e_{(T, \sigma)}(c, g) &:= \begin{cases}
        1 & g \in \mathrm{Anc}_T(\sigma(c)) \\
        0 & \text{else} \\
    \end{cases}
\end{align*}
where $\mathrm{Anc}_T(g)$ is the set of ancestors of a genome position $g \in G$ with respect to $T$. Therefore, the likelihood of a tree and cell attachment given the observed data $d$ and the error rates $\alpha$ and $\beta$ is
\begin{align*}
    \prod_{c \in C}\prod_{g \in G} \mathbb{P}_{(\alpha, \beta)}(D(c, g) = d(c, g) \mid E(c, g) = e_{(T, \sigma)}(c, g))
\end{align*}
Since pairs of $(c, g) \in C \times G$ with $d(c, g) = ?$ are ignored, their probabilities in the product above are always set to 1. As stated before, the goal is now to find the tuple $(T, \sigma, \alpha, \beta)$ with the maxmimum likelihood.

\section{Algorithm introduction and possible challenges}

\ac{SCITE} is a \ac{MCMC} algorithm. A Monte Carlo algorithm runs a random experiment that produces a possible solution to a problem and evaluates how well the generated solution solves the problem. This operation of generating and evaluating a solution is then repeated multiple times and the result is the best solution the algorithm has encountered. In theory, it would suffice if the experiment had a probability greater than zero to produce a good solution, but in order to improve the solution quality and to reduce the required repetitions, one would use an experiment that produces the best solutions with a higher probability than worse solutions and that can be repeated quickly. As the name implies, \ac{MCMC} algorithms simulate a Markov Chain to produce solutions. The advantage of using Markov Chains is that the next sample may depend on the previous one and the algorithm therefore only needs to introduce small changes to the solution. This is often faster than generating a new solution from scratch and if the current sample is already a good solution, the change may preserve some of its quality. However, a designer of a \ac{MCMC} algorithm has to make sure that the chain actually converges on the desired distribution.

\ac{SCITE} actually modifies and approximates the problem: Instead of producing a maxmimum likelihood 4-tuple of a tree $T$, cell attachment $\sigma$, and the error rates $\alpha$ and $\beta$, it only produces a maximum likelihood tuple of a tree $T$ and the error rate $\beta$, summed over all possible cell attachments and with a fixed $\alpha$. In one iteration, it proposes a change either to $T$ or to $\beta$. Changes to $\beta$ are just the addition of a normally distributed summand, but there are multiple possible changes to $T$: It may either be ``Prune and Reattach'' where a subtree is removed and attached to a node in the remaining tree, ``Label Swap'' where two nodes are swapped in the tree, or ``Subtree Swap'' where two complete subtrees are swapped. After the change has been proposed, it's likelihood is computed and it is stored if it's the most likely solution yet. The proposed change however is not always accepted as the next state of the Markov Chain. Instead, rejection sampling is used to make solutions with a higher likelihood more probable to be drawn.

Implementationwise, the complete iteration of proposing a change and evaluating it should be implemented as one continuous, perfectly pipelined block. If this is the case, a new iteration can be issued every clock cycle and multiple blocks can be chained after another, producing one long pipeline that finishes one step for every block in the chain ever clock cycle. There are however multiple possible challenges: First of all, there are random draws required in multiple different positions in the algorithm. The original implementation uses a single \ac{RNG} for the entirety of the algorithm, which makes every draw uncorrelated to the next. On an \ac{FPGA} however, there should be an individual \ac{RNG} for every draw in order to reduce dependencies across the design. Then however, we need to assure that the draws from the different \acp{RNG} are uncorrelated. Another challenge may be tree traversal: Both the tree modifications and the likelihood computations require knowledge of which are a node's ancestors and descendants. This has to be computed by traversing the tree and while it's runtime is bounded by the tree size, the usual runtime is much lower. However, dynamic runtimes might interfer with loop pipelining, which is one of the core sources of an \ac{FPGA}'s performance. Lastly, there are a lot of exponential and logarithmic functions in the likelihood computation. These are very resource intensive on \acp{FPGA} and we should try to reduce their use as much as possible.

\section{Planned Work}

\begin{figure}
    % Modified from https://git.cs.uni-paderborn.de/syssec/templates/exposee
    \begin{ganttchart}
        [today=0, %"TODAY" vertical line
        x unit=0.5cm, %extend sheets to 1cm
        ]{1}{18}
        \gantttitle[title label node/.append style={below left=7pt and -3pt}]{WEEKS:\quad1}{1}
        \gantttitlelist{2,...,18}{1} \\
        \ganttgroup{Initial Implementation}{1}{4} \\
        \ganttgroup{Verification \& Benchmarking}{2}{6} \\
        \ganttgroup{Optimization}{5}{13} \\
        \ganttgroup{Writing \& Proofreading}{13}{18}
    \end{ganttchart}
    \centering
    \caption{Worst-Case work schedule}
    \label{fig:worstschedule}
\end{figure}

The first task will be an initial implementation of the algorithm with Intel oneAPI. This implementation may be naive and therefore possibly inefficient, but it should be functional and correct. Next, we will need to set up the verification and benchmarking system to establish the baseline of the initial FPGA implementation and the reference implementation. It is unlikely that the design will produce exactly the same results as the reference implementation. We will therefore need to design and setup up a statistical system to verify the quality of the results. Once the foundations are established, we can begin to optimize. There is not much sense in planning this task since it heavily depends on the bottlenecks at hand, but it will probably result in repeated iterations of profiling, identification of bottlenecks, research in existing solutions, implementating existing or original ideas and analysing the changes. We will write-up the findings from all analysis stages, which will aid use in the last phase: Writing the thesis and proofreading it. There, we will describe which performance we could achieve, which bottlenecks we have encountered and how we have resolved them, and how templating aided uses in the process.

According to the regulations, the workload of the thesis is supposed to contain nine weeks of full time labor, which is equivalent to 18 weeks of half time labor. The deadline for handing the thesis in is five months, which is approximately equivalent to 21 to 22 weeks and leaves a buffer of three to four weeks. We assume that the initial implementation will take approximately four weeks since the original source code is rather voluminous, but it may be as little as two weeks if it proofs to be simpler. Benchmarking will be rather easy with our experience in profiling oneAPI FPGA designs, but since ew have no experience in statistical verification, we can not actually make a good prediction for the required time. For writing the thesis, other students have quoted approximately four to six weeks of half-time work for thesis writing and proofreading, which leaves approximately nine to thirteen weeks between the initial implementation and writing the thesis for optimization. The resulting worst-case work schedule is visualized in figure \ref{fig:worstschedule}. Lastly, we plan to use the following structure for the thesis:

\begin{itemize}
    \item Introduction
    \begin{itemize}
        \item Motivation
        \item Problem Formulation
        \item Algorithm Introduction
        \item Related Work
    \end{itemize}
    \item Implementation
    \begin{itemize}
        \item Notable Bottlenecks and Solutions
        \item Verification \& Benchmarking Framework
    \end{itemize}
    \item Results
    \begin{itemize}
        \item Quality
        \item Performance
        \item Impact of C++ Templating
    \end{itemize}
    \item Conclusion
\end{itemize}
